# -*- coding: utf-8 -*-
"""NPL Class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mqdA7iN0Z_0N18HEeo3mPnfxSdGzDQCp

##Part 1
"""

vocab = {}
word_encoding = 1
def word_encoder(text):
  global word_encoding

  words = text.lower().split(" ")
  encoding = []

  for word in words:
    if word in vocab:
      code = vocab[word]
      encoding.append(code)
    else:
      vocab[word] = word_encoding
      encoding.append(word_encoding)
      word_encoding += 1

  return encoding

text = "I thought this is a test sentence for NLP's Word encoding process but this is not"
encoding = word_encoder(text)
print(encoding)
print(vocab)

positive_review = "I thought the movie was going to be bad but it was actually amazing"
negative_review = "I thought the movie was going to be amazing but it was actually bad"


pos_encode = word_encoder(positive_review)
neg_encode = word_encoder(negative_review)

print("Positive:", pos_encode)
print("Negative:", neg_encode)

vocab

import tensorflow as tf
from tensorflow import keras
import numpy as np


sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog\'s is amazing?'
]

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 100, oov_token="<OOV>")
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)

print("\nWord Index = " , word_index)
print("\nSequences = " , sequences)

#To fixed input shape
padded = tf.keras.utils.pad_sequences(sequences, maxlen=5)

print("\nPadded Sequences:")
print(padded)


# # Try with words that the tokenizer wasn't fit to
test_data = [
    'i really love my dog',
    'my dog loves my manatee'
]

test_seq = tokenizer.texts_to_sequences(test_data)

print("\nTest Sequence = ", test_seq)

padded = tf.keras.utils.pad_sequences(test_seq, maxlen=10)
print("\nPadded Test Sequence: ")
print(padded)

tokenizer.sequences_to_texts(sequences)

"""##Part 2: Text Classification"""

VOCAB_SIZE = 10000
embedding_dim = 16
MAXLEN = 100
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"
training_size = 20000

!wget --no-check-certificate \
    https://storage.googleapis.com/learning-datasets/sarcasm.json \
    -O /tmp/sarcasm.json

import json

with open("/tmp/sarcasm.json", 'r') as f:
    datastore = json.load(f)

sentences = []
labels = []

for item in datastore:
    sentences.append(item['headline'])
    labels.append(item['is_sarcastic'])

training_sentences = sentences[0:training_size]
testing_sentences = sentences[training_size:]
training_labels = labels[0:training_size]
testing_labels = labels[training_size:]

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)

word_index = tokenizer.word_index

training_sequences = tokenizer.texts_to_sequences(training_sentences)
training_padded = tf.keras.utils.pad_sequences(training_sequences, maxlen=MAXLEN, padding=padding_type, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = tf.keras.utils.pad_sequences(testing_sequences, maxlen=MAXLEN, padding=padding_type, truncating=trunc_type)

testing_padded

# Convert numpy arrays to TensorFlow Tensors
training_padded = tf.convert_to_tensor(training_padded)
training_labels = tf.convert_to_tensor(training_labels)
testing_padded = tf.convert_to_tensor(testing_padded)
testing_labels = tf.convert_to_tensor(testing_labels)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(VOCAB_SIZE, embedding_dim, input_length=MAXLEN),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()

num_epochs = 30
history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)

import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_sentence(text):
    return ' '.join([reverse_word_index.get(int(i), '?') for i in text])

print(decode_sentence(training_padded[2]))
print(training_sentences[2])
print(labels[2])

sentence = ["granny starting to fear spiders in the garden might be real", "game of thrones season finale showing this sunday night"]
sequences = tokenizer.texts_to_sequences(sentence)
padded = tf.keras.utils.pad_sequences(sequences, maxlen=MAXLEN, padding=padding_type, truncating=trunc_type)
print(model.predict(padded))

"""##Part 3: Text Generation"""

!wget --no-check-certificate \
    https://storage.googleapis.com/learning-datasets/irish-lyrics-eof.txt \
    -O /tmp/irish-lyrics-eof.txt

tokenizer = tf.keras.preprocessing.text.Tokenizer()

data = open('/tmp/irish-lyrics-eof.txt').read()

corpus = data.lower().split("\n")

tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1

print(tokenizer.word_index)
print(total_words)

input_sequences = []
for line in corpus:
	token_list = tokenizer.texts_to_sequences([line])[0]
	for i in range(1, len(token_list)):
		n_gram_sequence = token_list[:i+1]
		input_sequences.append(n_gram_sequence)

# pad sequences
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(tf.keras.utils.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# create predictors and label
xs, labels = input_sequences[:,:-1],input_sequences[:,-1]

ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)

print(tokenizer.word_index['in'])
print(tokenizer.word_index['the'])
print(tokenizer.word_index['town'])
print(tokenizer.word_index['of'])
print(tokenizer.word_index['athy'])
print(tokenizer.word_index['one'])
print(tokenizer.word_index['jeremy'])
print(tokenizer.word_index['lanigan'])

print(xs[6])
print(ys[6])
print(xs[5])
print(ys[5])

model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(total_words, 100, input_length=max_sequence_len-1))
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)))
model.add(tf.keras.layers.Dense(total_words, activation='softmax'))
adam = tf.keras.optimizers.Adam(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')
history = model.fit(xs, ys, epochs=100, verbose=1)
#print model.summary()
print(model)

seed_text = "I've got a bad feeling about this"

next_words = 100

for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = tf.keras.utils.pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = np.argmax(model.predict(token_list), axis=-1)
	output_word = ""
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	seed_text += " " + output_word
	print(seed_text)
print(seed_text)


[0.25,0.25,0.50]

"""#Part 4: Sequence to Sequence"""

import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import pathlib
import random
import string
import re
import numpy as np

import tensorflow as tf
import keras
from keras import layers
from keras import ops
from keras.layers import TextVectorization

"""##English to Spanish"""

text_file = keras.utils.get_file(
    fname="spa-eng.zip",
    origin="http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip",
    extract=True,
    cache_dir="/content/",
)
# Construct the correct path to the extracted file
text_file = pathlib.Path(text_file).parent /"spa-eng_extracted" / "spa-eng" / "spa.txt"
print(f"Using data file: {text_file}")



with open(text_file, "r", encoding="utf8") as f:
    lines = f.read().split("\n")[:-1]
text_pairs = []
for line in lines:
    eng, spa = line.split("\t")
    spa = "[start] " + spa + " [end]"
    text_pairs.append((eng, spa))

# --- Hyperparameters ---
VOCAB_SIZE = 15000
SEQUENCE_LENGTH = 20
BATCH_SIZE = 64
EMBED_DIM = 256
NUM_HEADS = 8
FF_DIM = 2048
NUM_LAYERS = 4

# Shuffle for better training
random.shuffle(text_pairs)


# --- Split Dataset ---
num_val_samples = 2000
train_pairs = text_pairs[:-num_val_samples]
val_pairs = text_pairs[-num_val_samples:]

train_pairs[:5]

strip_chars = string.punctuation + "¿"
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

def custom_standardization(input_string):
    lowercase = tf.strings.lower(input_string)
    return tf.strings.regex_replace(
        lowercase, f"[{re.escape(strip_chars)}]", ""
    )

eng_vectorization = layers.TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode="int",
    output_sequence_length=SEQUENCE_LENGTH,
    standardize=custom_standardization,
)
spa_vectorization = layers.TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode="int",
    output_sequence_length=SEQUENCE_LENGTH + 1, # Ensure Spanish is also padded to SEQUENCE_LENGTH
    standardize=custom_standardization,
)

train_english_texts = [pair[0] for pair in train_pairs]
train_spanish_texts = [pair[1] for pair in train_pairs]

# Adapt the vectorization layers to the training data
eng_vectorization.adapt(train_english_texts)
spa_vectorization.adapt(train_spanish_texts)


def format_dataset(eng, spa):
    eng = eng_vectorization(eng)
    spa = spa_vectorization(spa)
    return (
        {
            "encoder_inputs": eng,
            "decoder_inputs": spa[:, :-1],
        },
        spa[:, 1:],
    )


def make_dataset(pairs):
    eng_texts, spa_texts = zip(*pairs)
    eng_texts = list(eng_texts)
    spa_texts = list(spa_texts)
    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.map(format_dataset)
    return dataset.cache().shuffle(2048).prefetch(16)


train_ds = make_dataset(train_pairs)
val_ds = make_dataset(val_pairs)

# To inspect a tf.data.Dataset, you need to iterate through it
for element in train_ds.take(1): # take(1) gets the first batch
    encoder_inputs, decoder_inputs = element[0]["encoder_inputs"], element[0]["decoder_inputs"]
    target_outputs = element[1]

    print("Encoder inputs shape:", encoder_inputs.shape)
    print("Decoder inputs shape:", decoder_inputs.shape)
    print("Target outputs shape:", target_outputs.shape)

"""##English to Bangla"""

from google.colab import drive
drive.mount('/content/drive')

def read_file(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            return content
    except FileNotFoundError:
        print(f"Error: The file {filepath} was not found.")
        return None
    except Exception as e:
        print(f"An error occurred: {e}")
        return None


en_text = read_file("/content/drive/MyDrive/Dataset/hasan-etal-2020-low/data/corpus.train.en")
bn_text = read_file("/content/drive/MyDrive/Dataset/hasan-etal-2020-low/data/corpus.train.bn")


en_text = en_text.split("\n")
bn_text = bn_text.split("\n")

text_pairs = []

for line in zip(en_text, bn_text):
    en, bn = line
    bn = "[start] " + bn + " [end]"
    text_pairs.append((en,bn))

random.shuffle(text_pairs)

num_train_samples = 30000
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples:num_train_samples+5000]

# --- Hyperparameters ---
VOCAB_SIZE = 15000
SEQUENCE_LENGTH = 100
BATCH_SIZE = 64
EMBED_DIM = 256
NUM_HEADS = 8
FF_DIM = 2048
NUM_LAYERS = 4

strip_chars = string.punctuation + "¿"
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

def custom_standardization(input_string):
    lowercase = tf.strings.lower(input_string)
    return tf.strings.regex_replace(
        lowercase, f"[{re.escape(strip_chars)}]", ""
    )

eng_vectorization = layers.TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode="int",
    output_sequence_length=SEQUENCE_LENGTH,
    standardize=custom_standardization,
)
bn_vectorization = layers.TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode="int",
    output_sequence_length=SEQUENCE_LENGTH + 1,
    standardize=custom_standardization,
)

train_english_texts = [pair[0] for pair in train_pairs]
train_bangla_texts = [pair[1] for pair in train_pairs]

# Adapt the vectorization layers to the training data
eng_vectorization.adapt(train_english_texts)
bn_vectorization.adapt(train_bangla_texts)


def format_dataset(eng, bn):
    eng = eng_vectorization(eng)
    bn = bn_vectorization(bn)
    return (
        {
            "encoder_inputs": eng,
            "decoder_inputs": bn[:, :-1],
        },
        bn[:, 1:],
    )


def make_dataset(pairs):
    eng_texts, bn_texts = zip(*pairs)
    eng_texts = list(eng_texts)
    bn_texts = list(bn_texts)
    # Convert lists to tensors before creating the dataset
    dataset = tf.data.Dataset.from_tensor_slices((tf.constant(eng_texts), tf.constant(bn_texts)))
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.map(format_dataset)
    return dataset.cache().shuffle(2048).prefetch(16)


train_ds = make_dataset(train_pairs)
val_ds = make_dataset(val_pairs)

# To inspect a tf.data.Dataset, you need to iterate through it
for element in train_ds.take(1): # take(1) gets the first batch
    encoder_inputs, decoder_inputs = element[0]["encoder_inputs"], element[0]["decoder_inputs"]
    target_outputs = element[1]

    print("Encoder inputs shape:", encoder_inputs.shape)
    print("Decoder inputs shape:", decoder_inputs.shape)
    print("Target outputs shape:", target_outputs.shape)

"""##Architecture"""

import keras.ops as ops


class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [
                layers.Dense(dense_dim, activation="relu"),
                layers.Dense(embed_dim),
            ]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, mask=None):
        if mask is not None:
            padding_mask = ops.cast(mask[:, None, :], dtype="int32")
        else:
            padding_mask = None

        attention_output = self.attention(
            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask
        )
        proj_input = self.layernorm_1(inputs + attention_output)
        proj_output = self.dense_proj(proj_input)
        return self.layernorm_2(proj_input + proj_output)

    def get_config(self):
        config = super().get_config()
        config.update(
            {
                "embed_dim": self.embed_dim,
                "dense_dim": self.dense_dim,
                "num_heads": self.num_heads,
            }
        )
        return config


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim

    def call(self, inputs):
        length = ops.shape(inputs)[-1]
        positions = ops.arange(0, length, 1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return ops.not_equal(inputs, 0)

    def get_config(self):
        config = super().get_config()
        config.update(
            {
                "sequence_length": self.sequence_length,
                "vocab_size": self.vocab_size,
                "embed_dim": self.embed_dim,
            }
        )
        return config


class TransformerDecoder(layers.Layer):
    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.latent_dim = latent_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [
                layers.Dense(latent_dim, activation="relu"),
                layers.Dense(embed_dim),
            ]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, mask=None):
        inputs, encoder_outputs = inputs
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is None:
            inputs_padding_mask, encoder_outputs_padding_mask = None, None
        else:
            inputs_padding_mask, encoder_outputs_padding_mask = mask

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=causal_mask,
            query_mask=inputs_padding_mask,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            query_mask=inputs_padding_mask,
            key_mask=encoder_outputs_padding_mask,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        proj_output = self.dense_proj(out_2)
        return self.layernorm_3(out_2 + proj_output)

    def get_causal_attention_mask(self, inputs):
        input_shape = ops.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = ops.arange(sequence_length)[:, None]
        j = ops.arange(sequence_length)
        mask = ops.cast(i >= j, dtype="int32")
        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = ops.concatenate(
            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],
            axis=0,
        )
        return ops.tile(mask, mult)

    def get_config(self):
        config = super().get_config()
        config.update(
            {
                "embed_dim": self.embed_dim,
                "latent_dim": self.latent_dim,
                "num_heads": self.num_heads,
            }
        )
        return config

# Define the Transformer model for English to Bangla
encoder_inputs = tf.keras.Input(shape=(None,), dtype="int64", name="encoder_inputs")
x = PositionalEmbedding(SEQUENCE_LENGTH, eng_vectorization.vocabulary_size(), EMBED_DIM)(encoder_inputs)
encoder_outputs = TransformerEncoder(EMBED_DIM, FF_DIM, NUM_HEADS)(x)
encoder = keras.Model(encoder_inputs, encoder_outputs)

decoder_inputs = tf.keras.Input(shape=(None,), dtype="int64", name="decoder_inputs")
x = PositionalEmbedding(SEQUENCE_LENGTH, bn_vectorization.vocabulary_size(), EMBED_DIM)(decoder_inputs)
x = TransformerDecoder(EMBED_DIM, FF_DIM, NUM_HEADS)([x, encoder_outputs])
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(bn_vectorization.vocabulary_size(), activation="softmax")(x)

transformer = keras.Model(
    {"encoder_inputs": encoder_inputs, "decoder_inputs": decoder_inputs},
    decoder_outputs,
    name="transformer_bn", # Give a different name to avoid conflict
)

transformer.summary()
transformer.compile(
    "rmsprop",
    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=0),
    metrics=["accuracy"],
)

transformer.fit(train_ds, epochs=30, validation_data=val_ds)

# Get the vocabulary for detokenization
bn_vocab = bn_vectorization.get_vocabulary()
bn_index_to_word = dict(enumerate(bn_vocab))
end_token_index = bn_vocab.index("[end]")

def decode_sequence(input_sentence,transformer):
    tokenized_input_sentence = eng_vectorization([input_sentence])

    # Initialize the target sequence with the [start] token
    decoded_sentence = "[start]"

    # Loop up to the maximum decoder input length
    for i in range(SEQUENCE_LENGTH - 1):
        # Prepare decoder input (current partial sequence)
        tokenized_target_sentence = bn_vectorization([decoded_sentence])[:, :-1]

        # Predict
        predictions = transformer.predict(
            {
                "encoder_inputs": tokenized_input_sentence,
                "decoder_inputs": tokenized_target_sentence,
            }
        )

        # Select the last token prediction and find the next word ID
        # We are predicting the next token based on the sequence so far,
        # which corresponds to the last token in the padded input sequence.
        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = bn_index_to_word[sampled_token_index]

        if sampled_token == "[end]":
            break

        # Append to the decoded sequence
        decoded_sentence += " " + sampled_token

    # Cleanup the output
    return decoded_sentence.replace("[start] ", "").replace(" [end]", "")

# Save the model, including custom layers in the native Keras format
transformer.save("transformer_model.keras")

from google.colab import files
files.download('transformer_model.keras')

# Load the model, providing the custom objects


loaded_transformer = keras.saving.load_model(
    "transformer_model.keras",
    custom_objects={
        "PositionalEmbedding": PositionalEmbedding,
        "TransformerEncoder": TransformerEncoder,
        "TransformerDecoder": TransformerDecoder,
        "not_equal": tf.keras.ops.not_equal,
    }
)

x = decode_sequence(" you are going home.",transformer)
x

