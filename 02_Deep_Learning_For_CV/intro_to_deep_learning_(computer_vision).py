# -*- coding: utf-8 -*-
"""Intro To Deep Learning (Computer Vision).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18UcCOKW5L5rsyqIPVlLpG2z8I9Wq15tk
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import clear_output

"""### Building a dataset"""

Sx = np.array([0, 1, 2.5, 3, 4, 5])
Sy = np.array([0.6, 0, 2, 2.2, 4.7, 5])

# Plotting in graph
plt.scatter(Sx, Sy)

# Graph axis names and grids
plt.grid(True)
plt.xlabel('Sx')
plt.ylabel('Sy')

"""Lets assume a line

$$y = mx + c$$

Where $m$ and $c$ are unknown, which we are trying to find.

We assume a random value for $m$ and $c$ ($m = 2$ and $c = 0$)
"""

m = tf.Variable(2, dtype=tf.float32)
c = tf.Variable(0, dtype=tf.float32)

def line_fn(x):
    return m*x + c

p = np.arange(0, 5, 0.1)
plt.plot(p, line_fn(p).numpy())

# Plotting in graph
plt.scatter(Sx, Sy)

# Graph axis names and grids
plt.grid(True)
plt.xlabel('Sx')
plt.ylabel('Sy')

"""## Gradient descending algorithm:
$$m_{t} = m_{t-1} - lr \; \frac{\partial \;\; loss(l(x), y)}{\partial m} $$

$$loss(l(x), y) = (l(x) - y)^2$$

#### Here,

* $t$ = Time step
* $x$ = Input
* $y$ = Output
* $m$ = Updatable variable
* $loss(\cdot, \cdot)$ = Loss function
* $lr$ = Learning rate
* $l(\cdot)$ = Line function

#### Partial derivatives:

$\frac{\partial \;\; loss(l(x), y)}{\partial m} = (l(x) - y)^2$
$ = (mx+c-y)^2$
$ = 2(mx+c-y)x$

$\frac{\partial \;\; loss(l(x), y)}{\partial c} = (l(x) - y)^2$
$ = (mx+c-y)^2$
$ = 2(mx+c-y)$
"""

# learning rate
lr = 0.01
total_steps = 100

for step in range(total_steps):
    print(f"Step {step+1:2}:")
    print("-"*30)

    with tf.GradientTape() as tape:
        # Printing value of the variables
        print(f"M: {m.numpy():.4f}, C: {c.numpy():.4f}")

        # Stating what variables need to be partially differentiated and calibrated
        tape.watch([m, c])

        # Passing the points to the line function
        pred_y = line_fn(Sx)

        # Calculating the difference/loss of the output (pred_y) of the function
        # w.r.t. the known output (Sy)
        loss = (pred_y - Sy) * (pred_y - Sy)


    # Calculating the gradients w.r.t. the partially diff. parameters
    # and the generated output loss
    grads = tape.gradient(loss, [m, c])

    # Showing the output just for educational purposs
    print(f"M_grad:, {grads[0].numpy():.2f}, C_grad: {grads[1].numpy():.2f}")

    # Updating the gradients
    m = m - lr * grads[0]
    c = c - lr * grads[1]

    print()

"""## Lets check the final result"""

p = np.arange(0, 5, 0.1)
plt.plot(p, line_fn(p).numpy())

# Plotting in graph
plt.scatter(Sx, Sy)

# Graph axis names and grids
plt.grid(True)
plt.xlabel('Sx')
plt.ylabel('Sy')

"""#### How can we get a line that goes through all the points given in the above graph?

We can not achieve this using a single straight line. What if we can combine multiple straight lines?

Lets assume a straight line

$$y = wx + c$$

We can build a polynomial by merging multiple straing lines using the the following equations:

$$
l_0 = \sigma (w_0*x + c_0) \\
l_1 = \sigma (w_1*l_0 + c_1) \\
l_2 = w_2 *l1 + c_2 \\
$$
Here,
$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

> $Y =(\sum_{i=0}^n w_i x_i) + b$
"""

# Reshape into 2D: (6 samples, 1 feature)
Sx = Sx.reshape(-1, 1).astype(np.float32)
Sy = Sy.reshape(-1, 1)

print(Sx.shape,Sy.shape)


W1 = tf.Variable(tf.random.normal([1, 4]), dtype=tf.float32)
b1 = tf.Variable(tf.zeros([4]), dtype=tf.float32)

W2 = tf.Variable(tf.random.normal([4, 4]), dtype=tf.float32)
b2 = tf.Variable(tf.zeros([4]), dtype=tf.float32)

W3 = tf.Variable(tf.random.normal([4, 1]), dtype=tf.float32)
b3 = tf.Variable(tf.zeros([1]), dtype=tf.float32)


def mlp(x):
    l0 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)
    l1 = tf.nn.sigmoid(tf.matmul(l0, W2) + b2)
    l2 = tf.matmul(l1, W3) + b3

    return l2

def plot_line():

    clear_output(wait=True)

    # Generate smooth input values and cast to float32
    p = np.arange(0, 5, 0.1).reshape(-1, 1).astype(np.float32)

    # Predict using the model
    pred = mlp(p)

    # Plot predictions (curve) and training data points
    plt.plot(p, pred.numpy(), label="Model Prediction")
    plt.scatter(Sx, Sy)

    plt.grid(True)
    plt.xlabel('Sx')
    plt.ylabel('Sy')
    plt.show()

plot_line()

"""## Gradient descending algorithm:
$$m_{t} = m_{t-1} - lr \; \frac{\partial \;\; loss(l(x), y)}{\partial m} $$

$$loss(l(x), y) = (l(x) - y)^2$$

#### Here,

* $t$ = Time step
* $x$ = Input
* $y$ = Output
* $m$ = Updatable variable
* $loss(\cdot, \cdot)$ = Loss function
* $lr$ = Learning rate
* $l(\cdot)$ = Line function
"""

lr = 0.1
epochs = 30000

# Training loop
for epoch in range(epochs):
    with tf.GradientTape() as tape:
        # Predictions
        y_pred = mlp(Sx)
        # Mean Squared Error loss
        loss = tf.reduce_mean((y_pred - Sy) ** 2)

    # Compute gradients w.r.t all weights and biases
    grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])

    # Update weights manually (Gradient Descent)
    W1.assign_sub(lr * grads[0])
    b1.assign_sub(lr * grads[1])
    W2.assign_sub(lr * grads[2])
    b2.assign_sub(lr * grads[3])
    W3.assign_sub(lr * grads[4])
    b3.assign_sub(lr * grads[5])

    if (epoch+1) % 1000 == 0:
        plot_line()
        print(f"Epoch {epoch+1}, Loss: {loss.numpy():.4f}")

plot_line()

line_fn(3.85)

# Example input (batch of 4 samples, each sample = sequence of length 10, feature size 8)
sample_input = tf.random.normal((4, 10, 8))

print("=== Dense Layer ===")
dense = tf.keras.layers.Dense(16, activation='relu')
print(dense(sample_input).shape)   # (4, 10, 16) → applied on last dim

print("\n=== Simple RNN Layer ===")
rnn = tf.keras.layers.SimpleRNN(32)
print(rnn(sample_input).shape)     # (4, 32) → returns last output by default

print("\n=== LSTM Layer ===")
lstm = tf.keras.layers.LSTM(32)
print(lstm(sample_input).shape)    # (4, 32)

print("\n=== GRU Layer ===")
gru = tf.keras.layers.GRU(32)
print(gru(sample_input).shape)     # (4, 32)

print("\n=== Bidirectional RNN ===")
bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))
print(bi_lstm(sample_input).shape) # (4, 64) → doubled (forward + backward)

# Example input for CNN (batch=4, image=28x28, channels=1)
image_input = tf.random.normal((4, 28, 28, 1))

print("\n=== Convolution Layer ===")
conv = tf.keras.layers.Conv2D(32, (3,3), activation='relu')
print(conv(image_input).shape)     # (4, 26, 26, 32)

print("\n=== MaxPooling Layer ===")
pool = tf.keras.layers.MaxPooling2D((2,2))
print(pool(conv(image_input)).shape)  # (4, 13, 13, 32)

print("\n=== Batch Normalization ===")
bn = tf.keras.layers.BatchNormalization()
print(bn(conv(image_input)).shape) # (4, 26, 26, 32)

"""#Basic classification

##Loading the Dataset
"""

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()

print(train_images.shape)
print(train_labels.shape)
print(test_images.shape)
print(test_labels.shape)


class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

train_labels[0]

"""The classes are:

Label	Description
- 0	: airplane
- 1	: automobile
- 2	: bird
- 3	: cat
- 4	: deer
- 5	: dog
- 6	: frog
- 7	: horse
- 8	" ship
- 9	: truck
"""

plt.figure()
plt.imshow(train_images[0])
plt.show()

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i])
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()

print(train_labels[0])

# Normalize pixel values to 0–1
train_images= train_images / 255.0
test_images = test_images / 255.0


# Convert labels to one-hot encoding
train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

# # Resize images (example: resize to 64x64)
# train_images = tf.image.resize(train_images, [64, 64])
# test_images = tf.image.resize(test_images, [64, 64])

print(train_images.shape, train_labels.shape)
print(test_images.shape, test_labels.shape)

"""Now Building the model


There are basically two ways to build a model

- Sequaltial
-Functional API
"""

model0 = tf.keras.models.Sequential()

  model0.add(tf.keras.layers.Flatten(input_shape=(32, 32, 3)))
  model0.add(tf.keras.layers.Dense(128, activation='relu'))
  model0.add(tf.keras.layers.Dense(64, activation='relu'))
  model0.add(tf.keras.layers.Dense(10, activation='softmax'))



  model1 = tf.keras.Sequential([
      tf.keras.layers.Flatten(input_shape=(32, 32, 3)),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(64,activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])

model0.summary()

input = tf.keras.layers.Input(shape=(32, 32, 3))

x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),strides=1,padding='same')(input)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation('relu')(x)

x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),strides=1)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation('relu')(x)

x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(10, activation='softmax')(x) #softmax for prediction

model = tf.keras.Model(inputs=input, outputs=output)

model.summary()

model.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#'adam"
    loss = tf.keras.losses.CategoricalCrossentropy(),
    metrics = [tf.keras.metrics.CategoricalAccuracy(),]
              #  tf.keras.metrics.F1Score(),
              #  tf.keras.metrics.Precision(),
              #  tf.keras.metrics.Recall()],

)

# model.compile(optimizer='adam',
#               loss='categorical_crossentropy',
#               metrics=['categorical_accuracy'])

history = model.fit(train_images, train_labels, epochs=100,
                    validation_data=(test_images, test_labels))

# Evaluate
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test Accuracy: {test_acc:.2f}")

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import seaborn as sns
import numpy as np



y_pred = model.predict(test_images)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(test_labels, axis=1)


print("Classification Report:\n")
print(classification_report(y_true, y_pred_classes, target_names=class_names))

# 3️⃣ Confusion Matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

plt.plot(history.history['categorical_accuracy'], label='Train Accuracy')
plt.plot(history.history['val_categorical_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
], name="data_augmentation")


# Input layer
inputs = tf.keras.layers.Input(shape=(32,32,3))

# Apply augmentation first
x = data_augmentation(inputs) #image should not be normalized first

# Normalize images
x = tf.keras.layers.Rescaling(1./255)(x)

x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),strides=2)(input)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation('relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)

x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),strides=2)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation('relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)

x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
output = tf.keras.layers.Dense(10, activation='softmax')(x)

model = tf.keras.Model(inputs=input, outputs=output)

model.summary()

model.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#'adam"
    loss = tf.keras.losses.CategoricalCrossentropy(),
    metrics = [tf.keras.metrics.CategoricalAccuracy(),]
)


history = model.fit(train_images, train_labels, epochs=100,
                    validation_data=(test_images, test_labels))

# Evaluate
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test Accuracy: {test_acc:.2f}")

plt.figure(figsize=(15, 5))

#accuracy
plt.subplot(1,2,1)
plt.plot(history.history['categorical_accuracy'], label='Train Accuracy')
plt.plot(history.history['val_categorical_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Classification Report
y_pred = model.predict(test_images)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(test_labels, axis=1)

print("Classification Report:\n")
print(classification_report(y_true, y_pred_classes, target_names=class_names))

# 3️⃣ Confusion Matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""Key Architectures
Here’s how to differentiate the most prominent ones:

- AlexNet (2012):

    First big breakthrough CNN.

    8 layers, simple architecture.

    Introduced ReLU & dropout.

    Much better accuracy than traditional ML.

- VGG (2014):

    Very deep, up to 19 layers.

    Uses small 3×3 filters stacked together.

    Easy to understand but huge in parameters.

- ResNet (2015):

    Introduced skip connections (residual learning).

    Allows training of extremely deep networks (50, 101, 152 layers).

    Solved the problem of vanishing gradients.

- Inception (GoogLeNet):

    Uses parallel filters of different sizes (1×1, 3×3, 5×5).

    Efficient, fewer parameters than VGG.

    Introduced “Inception modules.”

- DenseNet (2017):

    Every layer is connected to every other layer.

    Reuses features → fewer parameters & better gradient flow.

- Xception (2017):

    Extension of Inception.

    Replaces standard convolutions with depthwise separable convolutions (faster, more efficient).
"""

# Load pre-trained ResNet50 without top layer
base_model = tf.keras.applications.ResNet50(
    # weights='imagenet',
    include_top=False,
    input_shape=(32, 32, 3)
)

inputs = tf.keras.Input(shape=(32, 32, 3))
# Removed preprocess_input as images are already scaled
x = base_model(inputs, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
outputs = tf.keras.layers.Dense(10, activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)

# Compile
model.compile(optimizer='adam',
              loss='categorical_crossentropy', # Changed loss function
              metrics=['accuracy'])

model.summary()

history = model.fit(train_images, train_labels, epochs=10,
                    validation_data=(test_images, test_labels))

# Evaluate
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test Accuracy: {test_acc:.2f}")



# Load pre-trained ResNet50 without top layer
base_model = tf.keras.applications.ResNet50(
    weights='imagenet',
    include_top=False,
    input_shape=(32, 32, 3)
)


base_model.trainable = True

# Let's say we unfreeze from layer -20 onwards
for layer in base_model.layers[:-20]:
    layer.trainable = False



inputs = tf.keras.Input(shape=(32, 32, 3))
# Removed preprocess_input as images are already scaled
x = base_model(inputs, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
outputs = tf.keras.layers.Dense(10, activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)

# Compile
model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), #small lr
              loss='categorical_crossentropy', # Changed loss function
              metrics=['accuracy'])

model.summary()
base_model.summary()

